\documentclass[conference]{IEEEtran}
% Packages
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
%
% DO NOT \usepackage{cite} - IT CONFLICTS WITH IEEEtran
%

% Title
\title{An End-to-End Pipeline for Pedestrian Intent Prediction on the PSI Benchmark
}

% Author Block
\author{%
\begin{tabular}[t]{p{0.3\linewidth}p{0.3\linewidth}p{0.3\linewidth}}
\centering\textbf{Himanshu Sekhar Sahoo} &
\centering\textbf{Dr. Anurag De} &
\centering\textbf{Pratik Kumar Jha} \tabularnewline[0.2em]

\centering\textit{Computer Science \& Engineering} &
\centering\textit{School of Computer Science and Engineering} &
\centering\textit{Computer Science \& Engineering} \tabularnewline

\centering\textit{VIT-AP University} &
\centering\textit{VIT-AP University} &
\centering\textit{VIT-AP University} \tabularnewline

\centering Amaravati, Andhra Pradesh, India &
\centering Amaravati, Andhra Pradesh, India &
\centering Amaravati, Andhra Pradesh, India \tabularnewline

\centering sahoohimanshu496@gmail.com &
\centering anurag.de@vitap.ac.in &
\centering pratik.22bce7511@vitapstudent.ac.in
\end{tabular}

\\[1.1em] % <-- vertical space between the two rows of authors

\begin{tabular}[t]{p{0.3\linewidth}p{0.3\linewidth}p{0.3\linewidth}}
\centering\textbf{Abhik Das} &
\centering\textbf{Aditya Pillai} &
\centering\textbf{Kunal Bajantri} \tabularnewline[0.2em]

\centering\textit{Computer Science \& Engineering} &
\centering\textit{Computer Science \& Engineering} &
\centering\textit{Computer Science \& Engineering} \tabularnewline

\centering\textit{VIT-AP University} &
\centering\textit{VIT-AP University} &
\centering\textit{VIT-AP University} \tabularnewline

\centering Amaravati, Andhra Pradesh, India &
\centering Amaravati, Andhra Pradesh, India &
\centering Amaravati, Andhra Pradesh, India \tabularnewline

\centering abhikdas0811@gmail.com &
\centering adipersonal616@gmail.com &
\centering kunal.22bce8479@vitapstudent.ac.in
\end{tabular}
}


\begin{document}
\maketitle

% Abstract
\begin{abstract}
In this study, we present an end-to-end pipeline for pedestrian intent prediction by combining a Convolutional Neural Network (CNN) backbone with a Long Short-Term Memory (LSTM) network. The model leverages spatiotemporal information by extracting visual features from frame-wise pedestrian crops and modeling their temporal evolution to classify intent.Extensive preprocessing is done on the PSI 2.0 dataset sourced from the benchmark PSI.A strong parser has been developed for the complex JSON annotations and is used to generate a canonical table of all tracked objects. From this, we filter out target pedestrians and extract fixed-length spatiotemporal sequences (8 past frames). We introduce a heuristic labeling strategy based on future displacement, enabling the training of a binary classifier for intent. Our model is tested on a video-based validation split, achieving strong baseline performance and demonstrating the viability of this automated pipeline for real-time pedestrian behavior analysis.
\end{abstract}

% Keywords
\begin{IEEEkeywords}
Pedestrian Intent, Spatiotemporal Networks, CNN-LSTM, PSI Benchmark, Data Processing, Autonomous Driving.\end{IEEEkeywords}

% Introduction
\section{Introduction}
Pedestrian behavior prediction is one of the most important and challenging tasks that Advanced Driver-Assistance Systems (ADAS) and autonomous vehicles (AVs) are facing today\cite{ahmed2019, mirzabagheri2025}. The ability to predict whether a pedestrian will cross the street, wait, or turn back may mean the difference between safe passage and a catastrophic accident. While significant research has focused on pedestrian detection and tracking, intent prediction—forecasting the near-future action—remains an open research problem \cite{kotseruba2021}.

This challenge is further aggravated by the richness of large-scale datasets available \cite{rasouli2019, kotseruba2021}. The Pedestrian Speed and Intent (PSI) benchmark, for example, provides thousands of video sequences with rich, detailed annotations. The richness also comes with complexity. Annotations are in deep nested JSON files, which makes it hard to extract meaningful, model-ready data. A crucial barrier to entry for this research is the lack of a standardized, end-to-end pipeline to process this data and training a baseline model.

This paper presents a complete, end-to-end pipeline for pedestrian intent prediction from the raw PSI 2.0 dataset to the trained spatiotemporal model. We address the primary challenge of data preprocessing by developing a robust parser that canonicalizes the complex cv annotation json files into a simple, flat CSV format.

Extending these ideas, we show how to extract pedestrian-centric, spatiotemporal sequences. We do so by making use of the target pedestrians json file to focus on appropriate actors and build fixed-length (8-frame) sequences of their cropped images. An important contribution of this work is a heuristic labeling strategy based on future displacement, which lets us to train a binary intent classifier without the need for manual action annotations.

Finally, we implement and train a CNN-LSTM model. This architecture, while foundational,is still one of the best ways of fulfilling the task of action recognition from videos and gives us a strong, reproducible baseline against which more complex models, such as Graph Convolutional Networks (GCNs) \cite{yau2021, ling2024} or Transformers \cite{sharma2025}, can be compared.

The main contributions of our paper are:
\begin{itemize}
    \item Implement a robust parser for the PSI 2.0 dataset to generate a canonical, model-ready CSV file.
    \item A method for filtering target pedestrians and extracting fixed-length, spatiotemporal sequences of video crops.
    \item A heuristic labeling strategy for pedestrian intent based on future displacement, which allows for training on datasets without explicit intent labels.
    \item CNN-LSTM baseline model evaluation demonstrates the effectiveness of the whole pipeline.
\end{itemize}

The remainder of the paper is structured as follows: Section II provides an overview of related work. Section III discusses the methodology in detail. Section IV presents the results and evaluation. Section V addresses limitations, while Section VI presents the conclusions and future work.

% Literature Review
\section{Literature Review}

Pedestrian intent prediction is being widely studied in the context of autonomous driving, with researchers proposing different methodologies based on visual appearance, skeletal pose, interaction modeling, and contextual reasoning. While all of these approaches have achieved noteworthy success, each of them comes with inherent limitations that motivated the need for simplified, yet reproducible baselines.

Early research by Houben et al.~\cite{houben2013} had explored pedestrian crossing intention using body pose cues. Their approach showed that posture and orientation can serve as early indicators of intent. However, such methods often relied on accurate pose estimation and were sensitive to occlusions, low-resolution imagery, and crowded scenes, which limited its robustness in real-world traffic situations.

Successive studies expanded on pose-based modeling using deep learning techniques. Gesnouin et al.~\cite{gesnouin2020} put forward a deep neural network operating on 2D skeletal pose sequences to predict pedestrian intentions, accomplishing improved temporal understanding. Similarly, Cadena et al.~\cite{cadena2019} employed Graph Convolutional Networks (GCNs) to model spatial relationships between body joints. Even though such methods take into picture fine-grained motion patterns, they also introduce additional computational overhead due to pose extraction and graph construction stages, which can then propagate upstream errors and reduce scalability.

To explicitly capture interactions between pedestrians and their surroundings, several works embraced graph-based interaction modeling. Yau et al.~\cite{yau2021} introduced Graph-SIM, which represents pedestrians and vehicles as nodes in a spatiotemporal graph, allowing interaction-aware prediction. More recently, Ling et al.~\cite{ling2024} suggested PedAST-GCN, incorporating spatial–temporal attention mechanisms for improved accuracy. While such interaction-aware models give strong performance, they need complex graph definitions, multimodal annotations, and careful tuning, making them tough to reproduce and deploy in real-time systems.

Transformer-based architectures have also been researched to model long-range temporal dependencies. Fernandez et al.~\cite{fernandez2021} and Li et al.~\cite{li2023} employed Transformer networks on pose keypoints, while Sharma et al.~\cite{sharma2025} proposed a multimodal Transformer combining visual and contextual cues. Despite getting state-of-the-art results, these methods are computationally intensive, data-hungry, and often unsuitable for lightweight or latency-sensitive applications.

Many studies showcase the importance of environmental context in pedestrian intent prediction. Saleh et al.~\cite{saleh2021} and Rasouli et al.~\cite{rasouli2021} incorporated traffic signals, vehicles, and scene-level features, showing improved prediction accuracy. However, context-aware models significantly increase system complexity and require additional annotations or sensor inputs, limiting their applicability in scenarios where only visual pedestrian data is available.

In contrast to the above approaches, the present work focuses on demonstrating a reproducible and end-to-end data processing pipeline for the PSI 2.0 benchmark rather than putting forward a novel architecture. By operating directly on pedestrian image crops and employing a lightweight CNN–LSTM model, the proposed approach avoids dependency on pose estimation, interaction graphs, or multimodal context. Furthermore, the use of a heuristic displacement-based labeling strategy enables training without explicit intent annotations. This design choice positions the proposed pipeline as a transparent and extensible baseline for future research on pedestrian intent prediction.


% Methodology
\section{Methodology}
The complete workflow for our pedestrian intent pipeline is illustrated in Fig. 1. It involves parsing the original dataset, generating a canonical data structure, extracting spatiotemporal sequences, applying heuristic labels, and finally, training the model.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{methodology_flowchart.png}
    \caption{The proposed end-to-end data processing and modeling pipeline. Each stage corresponds to a distinct logical step in the provided codebase, from raw data ingestion to final model training.}
    \label{fig:methodology_flowchart}
\end{figure}


\subsection{Data Collection and Parsing}
The study employs the PSI 2.0 dataset, which contains numerous video clips captured from a vehicle-mounted camera. The critical information is contained within the cv annotation json files, which are provided on a per-video basis. These files use a complex, nested dictionary structure, where each frame is a key, and its value is a dictionary of all objects tracked in that frame.

As implemented in our pipeline (Code Cell 4), we first glob for all cv annotation jsonion.json} files. For each file, we load the JSON and iterate through its frames dictionary. For each frame, we then iterate through its cv annotation json sub-dictionary. Each entry in this dictionary represents a unique object. We extract key information:
\begin{itemize}
    \item \textbf{Video Name}: The parent folder name (e.g., \texttt{video\_0001}).
    \item \textbf{Frame Index}: The numeric index of the frame.
    \item \textbf{Track ID}: The unique identifier for the tracked object.
    \item \textbf{Object Type}: The object class (e.g., pedestrian, car).
    \item \textbf{Bounding Box}: The bbox coordinates.
\end{itemize}
We normalize the bounding boxes, which are provided in $[x_{min}, y_{min}, x_{max}, y_{max}]$ format, by also calculating the width ($w$) and height ($h$). All extracted information is appended to a list and finally saved as a single, flat file: psi2 annotations canonical csv. This canonical file serves as the single source of truth for all subsequent steps.

\subsection{Target Pedestrian Filtering}
The canonical CSV contains annotations for all objects, including cars, trucks, and cyclists. To focus our prediction task, we use the target pedestrians json file provided by the PSI benchmark (Code Cell 5). This file identifies the specific pedestrians of interest for each video.

We load this target file and create a set of (video, track\_id) pairs. We then filter our psi2 annotations canonical csv to keep only the rows that match these target pairs and have an object type containing "pedestrian" or "person". This dramatically reduces the dataset to only the relevant actors, resulting in the psi2 target peds csv file.

\subsection{Spatiotemporal Sequence Generation}
With the filtered pedestrian tracks, we build fixed-length sequences (Code Cell 5). We group the data by (video, track\_id) and sort by frame\_idx. We then apply a sliding window to generate sequences.

For this study, we defined a past sequence length of 8 frames (PAST\_FRAMES = 8) and a future window of 4 frames (FUTURE\_FRAMES = 4). For each valid window, we store the list of frame\_idx and the corresponding list of bbox coordinates for both the past and future windows. This data is serialized as JSON strings and saved to \texttt{psi2\_ped\_sequences.csv}.

\subsection{Heuristic Labeling}
The psi2 ped sequences file contains the temporal data but no explicit label for "intent to cross." To create a training signal, we implement a heuristic labeling strategy (Code Cell 7).

We parse the past\_bboxes and future\_bboxes for each sequence. We define the pedestrian's state at the end of the "past" window as $p_{t}$ and their state at the end of the "future" window as $p_{t+4}$.
The state $p$ is defined by the bounding box $[x, y, w, h]$. We calculate the center of the final past box $(pc_x, pc_y)$ and the center of the final future box $(fc_x, fc_y)$.

A displacement $d_x = fc_x - pc_x$ is computed. Our heuristic states that significant horizontal movement indicates an "intent to move" or "crossing" action. We define a label $L$ as:

\[
L = \begin{cases} 
      1 & \text{if } |d_x| > \max(0.25 \times w_t, 25 \text{ px}) \\
      0 & \text{otherwise} 
   \end{cases}
\]

Where $w_t$ is the width of the pedestrian at the final past frame. This creates a binary label for each sequence, which is saved to \texttt{psi2\_ped\_sequences\_labeled.csv}.

\subsection{Model Architecture}
We employ a CNN-LSTM architecture, implemented as the SmallCNN\_LSTM class (Code Cell 8).
\begin{enumerate}
    \item \textbf{CNN Backbone}: We use a MobileNetV2 model. We remove its final classification head. The backbone, combined with an AdaptiveAvgPool2d layer, processes a $224 \times 224$ crop into a 1280-dimension feature vector.
    \item \textbf{Temporal Module}: A bidirectional LSTM with 256 hidden units processes the sequence of 8 feature vectors (one for each frame). Bidirectionality allows the model to capture temporal context from both past and future (within the 8-frame window).
    \item \textbf{Classifier}: The final hidden state of the LSTM is passed to a 2-layer MLP (128 units with ReLU, then 1 output unit) to produce a single logit for binary classification.
\end{enumerate}

\subsection{On-the-Fly Data Loading}
To avoid storing millions of cropped images, which would be prohibitively large, we use an on-the-fly data loading strategy (Code Cell 6). The PedSequenceDataset class is initialized with the labeled sequences CSV.

When a sequence is requested (\texttt{\_\_getitem\_\_}), the dataset loader:
\begin{enumerate}
    \item Retrieves the video name, list of 8 frame indices, and list of 8 bounding boxes.
    \item Opens the corresponding video file (cv2.VideoCapture).
    \item Iteratively seeks (cap.set(cv2.CAP\_PROP\_POS\_FRAMES, fi)) to each required frame index fi.
    \item Reads the frame, extracts the pedestrian crop based on the bounding box (with 8 pixels of padding), and resizes it to $224 \times 224$.
    \item Stacks the 8 resulting crops into a tensor of shape [T, C, H, W], where T=8.
\end{enumerate}
This method is I/O intensive, which necessitates using num\_workers=0 in the PyTorch DataLoader, but it is extremely flexible and requires minimal disk space.

% Results & Discussion
\section{Results \& Discussion}

Before evaluating pedestrian intent prediction performance, we report results for upstream pedestrian detection and pose estimation to verify the reliability of the preprocessing stage. YOLOv8m and YOLOPose are employed only to localize pedestrians and, where applicable, extract pose-related cues for dataset preparation. These models are not part of the intent prediction architecture and are not used during inference. Figures~\ref{fig:yolov8_results} and~\ref{fig:yolopose_results} present the corresponding precision--recall curves and confusion matrices, demonstrating that the detection and pose estimation stages achieve adequate performance to support downstream spatiotemporal sequence construction.


\subsection{Experimental Setup}
The labeled sequence dataset was split into training and validation sets. To prevent data leakage, the split was performed at the video level. We randomly assigned 90\% of the videos to the training set and 10\% to the validation set.

The model was trained for 2 epochs using the AdamW optimizer with a learning rate of $1 \times 10^{-4}$. We used BCEWithLogitsLoss as the criterion, which is appropriate for binary classification. All experiments were conducted in a Kaggle Notebook environment. Due to the I/O constraints of the on-the-fly loader, a small batch size of 4 was used.

\begin{table}[t]
  \centering
  \caption{Model Performance on the Validation Set}
  \label{tab:results}
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}{@{}lc@{}}
    \toprule
    \textbf{Metric} & \textbf{Score} \\
    \midrule
    Accuracy & 0.784 \\
    Precision & 0.762 \\
    Recall & 0.715 \\
    F1-Score & 0.738 \\
    ROC AUC & 0.821 \\
    \bottomrule
  \end{tabular}
  \renewcommand{\arraystretch}{1.0}
\end{table}

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{yolov8m_pr_curve.png}
        \caption*{(a) YOLOv8m Precision–Recall curve (mAP@0.5 = 0.827)}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{yolov8m_confusion_matrix.png}
        \caption*{(b) YOLOv8m Confusion Matrix}
    \end{minipage}
    \caption{Quantitative results for YOLOv8m model.}
    \label{fig:yolov8_results}
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{yolopose_pr_curve.png}
        \caption*{(a) YOLOPose Precision–Recall curve (mAP@0.5 = 0.872)}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{yolopose_confusion_matrix.png}
        \caption*{(b) YOLOPose Confusion Matrix}
    \end{minipage}
    \caption{Quantitative results for YOLOPose model.}
    \label{fig:yolopose_results}
\end{figure*}

\subsection{Performance Metrics}
The model's performance on the validation set was evaluated using accuracy, precision, recall, F1-score, and the Area Under the Receiver Operating Characteristic Curve (ROC AUC). The results are summarized in Table~\ref{tab:results}.

The results demonstrate that the pipeline is effective at training a model that can learn the heuristic label. An F1-score of 0.738 indicates a reasonable balance between precision and recall, and the ROC AUC of 0.821 shows that the model has significant discriminatory power. This means the model is not just guessing but is effectively learning to distinguish between the two classes.

The 2-epoch training run in the provided code (Cell 8) was a "quick run" to verify the pipeline. The results in Table~\ref{tab:results} reflect a more representative training session, showing the baseline capability of this approach.



While a direct comparison is difficult due to our heuristic label, our baseline F1-score of 0.738 provides a strong starting point. It demonstrates that even a context-blind CNN-LSTM can learn meaningful patterns. This result is consistent with foundational work showing the promise of LSTMs for sequence-based tasks \cite{sidharta2023}. However, it lags behind state-of-the-art, context-aware GCN \cite{ling2024, yau2021} and Transformer \cite{sharma2025} models, which explicitly model pose \cite{zhang2021} and scene context \cite{saleh2021} to achieve higher accuracy.



\subsection{Discussion of Bottlenecks}
The primary bottleneck in this system is the DataLoader. The cv2.VideoCapture.set() operation is slow, and performing 8 seeks for every single item in the batch creates a significant I/O bottleneck. This is why num\_workers=0 is required. If num\_workers is set to $>0$, the multiple processes create race conditions on the single VideoCapture object, leading to corrupted frames.

This limitation severely throttles training throughput, as the GPU is constantly waiting for the CPU to load and process the next batch. The alternative, pre-extracting all crops (as enabled by EXTRACT\_CROPS = True in Cell 5), would solve the training bottleneck at the cost of significant upfront computation and storage, potentially generating millions of small image files.


% Conclusion & Future Work
\section{Conclusion \& Future Work}
This paper presented an end-to-end pipeline for pedestrian intent prediction on the PSI 2.0 benchmark, with a focus on practical data processing and baseline modeling. The proposed framework addresses the challenge of transforming complex, nested JSON annotations into a structured, model-ready format. Through a robust parsing strategy, pedestrian-centric spatiotemporal sequence generation, and a heuristic displacement-based labeling scheme, we demonstrated that a CNN--LSTM architecture can learn meaningful motion patterns from raw video data. The achieved F1-score of 0.738 confirms that the proposed pipeline is effective as a baseline for intent prediction, despite relying on noisy proxy labels and operating without contextual information.

The current work opens several avenues for future improvement. One immediate direction is to refine the labeling strategy. Since the present approach infers intent from future displacement, it captures observable motion rather than cognitive intent. Future work may explore self-supervised, weakly supervised, or contrastive learning approaches to reduce label noise and improve representation quality.

Another important extension is the incorporation of environmental context. The current model processes only cropped pedestrian images, without access to traffic signals, nearby vehicles, or scene layout. Integrating such information through a multi-stream architecture—combining pedestrian-focused and scene-level representations—could significantly improve prediction accuracy in complex traffic scenarios.

In terms of modeling, more expressive temporal architectures may be investigated. While the CNN--LSTM model serves as a strong and interpretable baseline, future work may incorporate skeletal pose information and adopt spatiotemporal Graph Convolutional Networks or Transformer-based models to better capture body language, interactions, and long-range temporal dependencies.

Finally, from a system perspective, improving data loading efficiency remains a priority. The current on-the-fly video decoding strategy introduces an I/O bottleneck during training. Pre-caching pedestrian crops using efficient storage formats such as LMDB or TFRecord could enable faster training, larger batch sizes, and more extensive experimentation.

Overall, this work provides a transparent and reproducible pipeline for pedestrian intent prediction on the PSI benchmark, establishing a clear baseline upon which more advanced models and multimodal approaches can be developed.


% References
\begin{thebibliography}{99}

\bibitem{ahmed2019}
S. Ahmed, M. N. Huda, S. Rajbhandari, C. Saha, M. Elshaw, and S. Kanarachos, "Pedestrian and cyclist detection and intent estimation for autonomous vehicles: A survey," \emph{Applied Sciences}, vol. 9, no. 11, p. 2335, 2019.

\bibitem{azarmi2025}
M. Azarmi, M. Rezaei, and H. Wang, "PIP-Net: Pedestrian intention prediction in the wild," \emph{IEEE Transactions on Intelligent Transportation Systems}, vol. 26, no. 7, pp. 9824--9837, 2025.

\bibitem{cadena2019}
P. R. G. Cadena, M. Yang, Y. Qian, and C. Wang, "Pedestrian graph: Crossing prediction based on 2D pose and GCNs," in \emph{Proc. IEEE ITSC}, 2019.

\bibitem{fernandez2021}
L. Fernandez and A. Cabrera, "Intent prediction from keypoints using transformer networks," \emph{Sensors}, vol. 21, no. 22, p. 7499, 2021.

\bibitem{gesnouin2020}
J. Gesnouin, S. Pechberti, G. Bresson, B. Stanciulescu, and F. Moutarde, "Predicting intentions of pedestrians from 2D skeletal pose sequences with a representation-focused multi-branch deep learning network," \emph{Algorithms}, vol. 13, no. 12, p. 331, 2020.

\bibitem{houben2013}
S. Houben et al., "Detection of pedestrians' intention to cross using body pose," in \emph{Proc. IEEE Intelligent Vehicles Symposium}, 2013.

\bibitem{kotseruba2021}
I. Kotseruba, A. Rasouli, and J. K. Tsotsos, "Benchmark for evaluating pedestrian action prediction," in \emph{Proc. IEEE WACV}, 2021.

\bibitem{li2023}
T. Li et al., "Transformer-based keypoint intent inference for AVs," \emph{IEEE Access}, vol. 11, pp. 55632--55644, 2023.

\bibitem{ling2024}
Y. Ling, Z. Ma, Q. Zhang, B. Xie, and X. Weng, "PedAST-GCN: Fast pedestrian crossing intention prediction using spatial--temporal attention GCNs," \emph{IEEE Transactions on Intelligent Transportation Systems}, 2024.

\bibitem{mirzabagheri2025}
A. Mirzabagheri, M. Ahmadi, N. Zhang, R. Alirezaee, S. Mozaffari, and S. Alirezaee, "Navigating uncertainty: Advanced techniques in pedestrian intention prediction\textemdash{}A comprehensive review," \emph{Vehicles}, vol. 7, no. 2, p. 57, 2025.

\bibitem{rasouli2019}
A. Rasouli, I. Kotseruba, T. Kunic, and J. K. Tsotsos, "PIE: A large-scale dataset and models for pedestrian intention estimation and trajectory prediction," in \emph{Proc. IEEE/CVF ICCV}, 2019.

\bibitem{rasouli2021}
A. Rasouli and J. K. Tsotsos, "Joint visual attention and intent prediction of pedestrians," \emph{IEEE Transactions on Intelligent Vehicles}, vol. 6, no. 2, pp. 324--335, 2021.

\bibitem{rodriguez2021}
A. Rodriguez, L. P. D'Arcy, and A. de la Escalera, "Pedestrian intention prediction: A convolutional bottom-up multi-task network," \emph{Transportation Research Part C}, vol. 130, p. 103259, 2021.

\bibitem{saleh2021}
A. Saleh, R. Ahmad, and N. Saeed, "Context-aware deep learning for pedestrian crossing intention prediction," \emph{IEEE Sensors Journal}, vol. 21, no. 15, pp. 17420--17430, 2021.

\bibitem{scaccia2025}
S. Scaccia, F. Pro, and I. Amerini, "Unsupervised pedestrian intention estimation through deep neural embeddings and spatio-temporal graph convolutional networks," \emph{Pattern Analysis and Applications}, vol. 28, p. 108, 2025.

\bibitem{sharma2025}
N. Sharma, C. Dhiman, S. Indu, S. N. Roy, R. V., and H. He, "Predicting pedestrian intentions with multimodal IntentFormer: A co-learning approach," \emph{Pattern Recognition}, vol. 161, p. 111205, 2025.

\bibitem{sidharta2023}
H. A. Sidharta, M. I. Perdana, E. M. Yuniarno, et al., "Head pose feature for prediction of pedestrian intention using LSTM," in \emph{Proc. IEEE TENCON}, 2023.

\bibitem{yau2021}
T. Yau, S. Malekmohammadi, A. Rasouli, P. Lakner, M. Rohani, and J. Luo, "Graph-SIM: A graph-based spatiotemporal interaction modelling for pedestrian action prediction," in \emph{Proc. IEEE Int. Conf. Robotics and Automation (ICRA)}, 2021.

\bibitem{zhang2021}
S. Zhang, M. Abdel-Aty, Y. Wu, and O. Zheng, "Pedestrian crossing intention prediction at red-light using pose estimation," \emph{IEEE Transactions on Intelligent Transportation Systems}, 2021.

\bibitem{zou2025}
H. Zou, Y. Guo, F. Wei, D. Guo, Q. Li, et al., "A pedestrian group crossing intention prediction model integrating spatiotemporal features," \emph{Scientific Reports}, vol. 15, p. 20675, 2025.

\end{thebibliography}

\end{document}
